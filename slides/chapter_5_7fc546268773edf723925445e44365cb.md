---
title: The importance of variable selection
key: 7fc546268773edf723925445e44365cb

---
## The importance of variable selection

```yaml
type: TitleSlide
key: 780f9acfbf
```





`@lower_third`
name: Frees
title: Instructor




---
## The importance of variable selection

```yaml
type: FullSlide
key: 67a0f8cd66
```

`@part1`
- With too many or too few variables, $s$ is too large an estimate of $\sigma$.
    - Prediction intervals are too large
    - Standard errors for the partial slopes are too large
- With too few or incorrect variables, we produce biased estimates of the slopes $\beta$. Thus, our predictions are biased and hence inaccurate.






`@script`




---
## Example. Regression using one explanatory variable

```yaml
type: FullSlide
key: 19d016d2fa
```

`@part1`
- **Too Many Variables**
    - The "true" model is $y_i = \beta_0+ \varepsilon_i$
    - We mistakenly use $y_i = \beta_0+ \beta_1 x_i^* + \varepsilon_i$
    - The prediction at a generic level $x$ is $b_0 + b_1^* x$.
    - It is not to hard to confirm that $$Bias   =   \mathrm{E} (b_0 + b_1^* x) - \mathrm{E } y= 0$$.






`@script`




---
## Example. Regression using one explanatory variable

```yaml
type: FullSlide
key: 426dc9a043
```

`@part1`
- **Too Few Variables**
    - The "true" model is   $y_i = \beta_0 + \beta_1 x_i + \varepsilon_i$.
    - We mistakenly use  $y_i = \beta_0^* \varepsilon_i$.
    - Under the true model,  $\overline{y} = \beta_0 + \beta_1 \overline{x} + \overline{\varepsilon}$
    - Thus, the bias is

$$
Bias  = \text{E }\bar{y} - \text{E }(\beta_0 + \beta_1 x + \varepsilon) 
$$

$$
= \text{E }(\beta_0 + \beta_1 \bar{x}+\bar{\varepsilon})-(\beta_0+\beta_1 x)=\beta_1 (\bar{x}-x).
$$

There is a persistent, long-term error in omitting the explanatory variable $x$.





`@script`




---
## Principle of parsimony

```yaml
type: FullSlide
key: 7856d60859
```

`@part1`
- The principle of parsimony, also known as *Occam's Razor*, states that when there are several possible explanations for a phenomenon, use the simplest.
    - A simpler explanation is easier to interpret.
    - Simpler,  "more parsimonious" models, do well on fitting out-of-sample data
    - Extraneous variables can cause problems of collinearity, leading to difficulty in interpreting individual coefficients.
- In contrast, in a quote often attributed to Albert Einstein, we should use "the simplest model possible, but no simpler."
    - Omitting important variables can lead to biased results
    - Including extraneous variables still yields good predictions 





`@script`




---
## Let's Practice

```yaml
type: FinalSlide
key: 0f52a6f2cd
```






`@script`




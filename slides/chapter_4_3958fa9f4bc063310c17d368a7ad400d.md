---
title: Unusual observations
key: 3958fa9f4bc063310c17d368a7ad400d

---
## Unusual observations

```yaml
type: TitleSlide
key: 016e8a625f
```





`@lower_third`
name: Frees
title: Instructor




---
## Unusual observations

```yaml
type: FullSlide
key: 3cf4afb8b5
```

`@part1`
- Regression coefficients can be expressed as (matrix) weighted averages of outcomes
    - Averages, even weighted averages can be strongly influenced by unusual observations
- Observations may be unusual in the *y* direction or in the *X* space
- For unusual in the *y* direction, we use a residual $e = y - \hat{y}$
    - By subtracting the fitted value $\hat{y}$, we look to the *y* distance from the regression plane
    - In this way, we "control" for values of explanatory variables





`@script`




---
## Standardized residuals

```yaml
type: FullSlide
key: b8e806195b
```

`@part1`
We standardize residuals so that we can focus on relationships of interest and achieve carry-over of experience from one data set to another.

Three commonly used definitions of standardize residuals are:

$$
\text{(a) }\frac{e_i}{s}, \text{ (b) }\frac{e_i}{s\sqrt{1-h{ii}}}, \text{(c)}\frac{e_i}{s{(i)}\sqrt{1-h{ii}}}.
$$

- First choice is simple
- Second choice, from theory, $\text{Var}(e_i)=\sigma ^{2}(1-h{ii}).$ Here, $h{ii}$ is the *i*th *leverage* (defined later).
- Third choice is termed "studentized residuals". 

Idea: numerator is independent of the denominator.





`@script`




---
## Outlier - an unusual standardized residual

```yaml
type: FullSlide
key: 193c40a4f8
```

`@part1`
- An *outlier* is an observation that is not well fit by the model; these are observations where the residual is unusually large.
- Unusual means what?  Many packages mark a point if the |standardized residual| > 2.
- Options for handling outliers
    - Ignore them in the analysis but be sure to discuss their effects.
    - Delete them from the data set (but be sure to discuss their effects).
    - Create a binary variable to indicator their presence. (This will increase your $R^2$!)





`@script`




---
## Influential points

```yaml
type: FullSlide
key: 2dd10c2c4b
```

`@part1`
- *Influential points* are observations that potentially have a lot to say about a regression fit. They may have large residuals, high leverage, or both.
- Intuitively, observations that are "far away" in the $x$-space have greater influence.
- One can get a feel for influential observations by looking a summary statistics (mins, maxs) for each explanatory variable.
- However, this is not sufficient - see the following graph .
    - The ellipsoid represents most of the data. 
    - The arrow marks an unusual point that is not unusual for $x_1$ or $x_2$.





`@script`




---
## High leverage point graph

```yaml
type: FullImageSlide
key: 1440b16ddf
```

`@part1`
![](https://assets.datacamp.com/production/repositories/2610/datasets/e1c25a0bc2ed021b66e7a333ef7cd02b41e35416/Ch4HighLeverage.png)





`@script`




---
## Leverage

```yaml
type: FullSlide
key: 6b63c2bca3
```

`@part1`
- Using matrix algebra, one can express the *i*th fitted value as a linear combination of observations

$
\hat{y}_{i} = h{i1} y{1} + \cdots+h{ii}y{i}+\cdots+h{in}y{n}.
$

- The term $h_{ii}$ is known as the *i*th leverage
    - The larger the value of $h_{ii}$, the greater the effect of the *i*th observation $y_i$ on the *i*th fitted value $\hat{y}_i$.
    - Statistical routines have values of the leverage coded, so computing this quantity. The key thing to know is that  $h_{ii}$ is based solely on the explanatory variables. If you change the $y$ values, the leverage does not change.
    - As commonly used rule of thumbs, a leverage is deemed to be "unusual" if its value exceeds three times the average (= number of regression coefficients divided by the number of observations.)





`@script`




---
## Final Slide

```yaml
type: FinalSlide
key: 597c977730
```









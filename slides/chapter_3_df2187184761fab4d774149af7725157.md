---
title: Hypothesis testing
key: df2187184761fab4d774149af7725157

---
## Hypothesis testing

```yaml
type: TitleSlide
key: c72bdd6346
```





`@lower_third`
name: Frees
title: Instructor

`@script`




---
## Testing the significance of a categorical variable

```yaml
type: FullCodeSlide
key: 0d10f3d138
```

`@part1`
```
> Term_mlr1 <- lm(logface ~ logincome + education + numhh + as.factor(Term4$marstat), data = Term4)
> anova(Term_mlr1)
Analysis of Variance Table

Response: logface
                          Df Sum Sq Mean Sq F value      Pr(>F)    
logincome                  1 222.63 222.629  97.280   < 2.2e-16 ***
education                  1  51.50  51.502  22.504 0.000003407 ***
numhh                      1  54.34  54.336  23.743 0.000001883 ***
as.factor(Term4$marstat)   2  14.81   7.406   3.236     0.04085 *  
Residuals                269 615.62   2.289                        

Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

> Term_mlr2 <- lm(logface ~ logincome + education + numhh, data = Term4)
> Fstat <- (anova(Term_mlr2)$`Sum Sq`[4] - anova(Term_mlr1)$`Sum Sq`[5])/
           (2*anova(Term_mlr1)$`Mean Sq`[5])
> Fstat
> cat("p-value is", 1 - pf(Fstat, df1 = 2 , df2 = anova(Term_mlr1)$Df[5]))

[1] 3.236048
p-value is 0.04085475

```





`@script`




---
## Overview of the general linear hypothesis

```yaml
type: FullSlide
key: d62c12b705
```

`@part1`
- The *likelihood ratio* is a general statistical test procedure that compares a model to a subset
- The *general linear hypothesis* test procedure is similar.
    - Start with a (large) linear regression model, examine the fit to a set of data
    - Compare this to smaller model that is a subset of the large model.
    - "Subset" is the sense that regression coefficients from the small model are linear combinations of regression coefficients of the large model (e.g., set them to zero)
- Although the likelihood ratio test is more generally available, the general linear hypothesis test is more accurate for smaller data sets (for normally distributed data)





`@script`




---
## Procedure for conducting the general linear hypothesis

```yaml
type: FullSlide
key: 29beebce29
```

`@part1`
- Run the full regression and get the error sum of squares and mean square error, which we label as $(Error SS)\_{full}$ and $s^2_{full}$, respectively.
- Run a reduced regression and get the error sum of squares, labelled $(Error SS)_{reduced}$.
- Using $p$ for the number of linear restrictions, calculate

$$
F-ratio = \frac{(Error SS)\_{reduced}-(Error SS)\_{full}}{p s^2\_{full}} .
$$
- The probability value is $p-value = \Pr(F\_{p,df} > F-ratio)$ where $F_{p,df}$ has an *F* distribution with  degrees of freedom *p* and  *df*, respectively. (Here, *df* is the degrees of freedom for the full model.)





`@script`




---
## The general linear hypothesis for a single variable

```yaml
type: FullSlide
key: 43db01e58e
```

`@part1`
- Suppose that you wish to test the hypothesis that a $\beta$ equals 0.
    - One could use the general linear hypothsis procedure with $p=1$.
    - One could also examine the corresponding $t-ratio$.
    - Which is correct?
-  **Both**. One can show that $(t-ratio)^2 = F-ratio$, so they are equivalent statistics.
    - The general linear hypothesis is useful because it can be extended to multiple coefficients.
    - The *t-ratio* is useful because it can be used to examine one-sided alternative hypotheses.





`@script`




---
## Let's Practice!

```yaml
type: FinalSlide
key: 588bae97b3
```






`@script`




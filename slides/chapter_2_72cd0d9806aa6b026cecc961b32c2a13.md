---
title: Understanding variability
key: 72cd0d9806aa6b026cecc961b32c2a13

---
## Understanding variability

```yaml
type: TitleSlide
key: 99cbf6a2d6
```





`@lower_third`
name: Frees
title: Instructor




---
## Understanding variability

```yaml
type: FullImageSlide
key: 33ac2934b8
```

`@part1`
![](https://assets.datacamp.com/production/repositories/2610/datasets/9ef67e0d61af6676df0dd2f4dee6a94cf7e7827a/Ch2UnderstandVariability.png)








---
## R script for visualizing the uncertainty about a line

```yaml
type: FullCodeSlide
key: 372a4d7d42
```

`@part1`
```
par(mar=c(2.2,2.1,.2,.2),cex=1.2)
x <- seq(-4, 4, len=101)
y <- x
plot(x, y, type = "l", xlim=c(-3, 4), xaxt="n", yaxt="n", xlab="", ylab="")
axis(1, at = c(-1, 1),lab = expression(bar(x), x))
axis(2, at = c(-1, 1, 3),lab = expression(bar(y), hat(y), y), las=1)
abline(-1, 0, lty = 2)
segments(-4, 1, 1, 1, lty=2)
segments(-4, 3, 1, 3, lty = 2)
segments(1, -4, 1, 3, lty = 2)
segments(-1, -4, -1, -1, lty = 2)

points(1, 3, cex=1.5, pch=19)

arrows(1.0, 1, 1.0, 3, code = 3, lty = 1, angle=15, length=0.12, lwd=2)
text(1.3, 2.2,   expression( y-hat(y)),cex=0.8) 
text(-.3,2.2,"'unexplained' deviation", cex=.8) 
arrows(1.0, -1, 1.0, 1, code = 3, lty = 1, angle=15, length=0.12, lwd=2)
text(1.85, 0, expression(hat(y)-bar(y) == b[1](x-bar(x)) ), cex=0.8 )
text(2.1, -0.5, " 'explained' deviation", cex=0.8)
arrows(-1, -1.0, 1, -1.0, code = 3, lty = 1, angle=15, length=0.12, lwd = 2)
text(0, -1.3, expression( x-bar(x)), cex=0.8  )
text(3.5, 2.7, expression( hat(y)== b[0]+ b[1]*x), cex=0.8  )
```








---
## The coefficient of determination, $R^2$

```yaml
type: FullSlide
key: adee321d8b
```

`@part1`
- After a little algebraic manipulation, this yields

$$\sum_i ( y_i -\bar{y})^2 
= \sum_i  (y_i - \hat{y}_i)^2 + \sum_i (\hat{y}_i- \bar{y})^2,$$

or  $Total~SS=Error~SS+Regression~SS$ where $SS$ stands for sum of
squares.
- Summarize with "$R$-square,'' the *coefficient of determination*, defined as

$$
R^2 = \frac{Regression~SS}{Total~SS}.
$$





`@script`




---
## The mean square error, $s^2$

```yaml
type: FullSlide
key: 9491753f63
```

`@part1`
- Define the estimate of the disturbance term $\epsilon_i = y_i - \left( \beta_0 + \beta_1 x_i \right),$

$$
e_i = y_i - \left( b_0 + b_1 x_i \right)
$$
the $i$th residual.
-  If we could observe disturbances, then we would estimate
$\sigma^2$ using $(n-1)^{-1}\sum_{i=1}^{n}\left( \varepsilon
_i-\overline{\varepsilon }\right)^2$.
- Instead, an estimator of $\sigma^2$, the *mean square error (MSE)*, is defined a

$$
s^2=\frac{1}{n-2}\sum_{i=1}^{n}e_i{}^2.
$$

- The square root, *s*, is known as the *residual standard error*





`@script`




---
## ANOVA Table

```yaml
type: TwoRows
key: ca3ca70afd
```

`@part1`
```
model_blr <- lm(sales ~ pop, data = Lot)
anova(model_blr)


sqrt(anova(model_blr)$Mean[2])
summary(model_blr)$r.squared
```

`@part2`
```
Analysis of Variance Table
Response: sales
          Df     Sum Sq    Mean Sq F value    Pr(>F)    
pop        1 2527165015 2527165015  175.77 < 2.2e-16 ***
Residuals 48  690116755   14377432
                       
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

[1] 3791.758
[1] 0.7854969
```




`@script`




---
## Let's Practice

```yaml
type: FinalSlide
key: 06363a7a1b
```








